{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Reconstruction\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2').to(device)\n",
    "model.eval()\n",
    "\n",
    "sent_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", device=device)\n",
    "sent_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class COCOCaptionsDataset(Dataset):\n",
    "    def __init__(self, caption_file):\n",
    "        self.coco = COCO(caption_file)\n",
    "        self.coco_data = []\n",
    "\n",
    "        for i, data_dict in self.coco.anns.items():\n",
    "            self.coco_data.append(data_dict['caption'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coco_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.coco_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.54s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco_dataset = COCOCaptionsDataset(\"Data/coco_captions/captions_train2017.json\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renconstruction Training (Sentence Embedding -> Sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lm(model, tokenizer, caps):\n",
    "\n",
    "    batch_size = len(caps)\n",
    "\n",
    "    targets = tokenizer(caps, padding=True, return_tensors='pt', return_attention_mask=True)\n",
    "    targets_ids = targets['input_ids'].to(device)\n",
    "    targets_mask = targets['attention_mask'].to(device)\n",
    "\n",
    "    token_embs = model.transformer.wte(targets_ids)\\\n",
    "    \n",
    "    sent_embs = sent_model.encode(caps, convert_to_tensor=True)\n",
    "\n",
    "    input_clip_embs = torch.zeros((token_embs.size(0), token_embs.size(1)+1, token_embs.size(2)), device=device)\n",
    "    target_clip_mask = torch.zeros((targets_mask.size(0), targets_mask.size(1)+1), dtype=torch.int64, device=device)\n",
    "    target_clip_ids = torch.zeros((targets_ids.size(0), targets_ids.size(1)+1), dtype=torch.int64, device=device)\n",
    "\n",
    "    input_clip_embs = torch.cat((sent_embs.unsqueeze(1), token_embs), dim=1) # Add input CLIP embedding\n",
    "    target_clip_ids = torch.cat((torch.zeros((batch_size, 1)).to(device), targets_ids), dim=1)    # Add dummy token; is ignored in loss \n",
    "    target_clip_mask = torch.cat((torch.ones((batch_size, 1)).to(device), targets_mask), dim=1)  # Avoid masking new token\n",
    "\n",
    "    outputs = model(\n",
    "        inputs_embeds=input_clip_embs,\n",
    "        return_dict=True,\n",
    "        output_hidden_states=True,\n",
    "        attention_mask=target_clip_mask\n",
    "    )\n",
    "\n",
    "    loss = torch.nn.functional.cross_entropy(outputs['logits'][:, :-1].reshape(-1, outputs['logits'].size(-1)), targets_ids.flatten(), ignore_index=0)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def tune_caption_to_CLIP(model, tokenizer, optimizer, scheduler, dataloader, epochs=5):\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "       \n",
    "        print(f\"Training epoch: {epoch}\")\n",
    "        num_batches = len(dataloader)\n",
    "\n",
    "        for batch_idx, caps in  tqdm(enumerate(dataloader)):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = train_lm(model, tokenizer, caps)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            if batch_idx % 1000 == 0 and batch_idx != 0:\n",
    "                print(f\"Loss at batch {batch_idx} / {num_batches}  = {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8693f65a9d4d128e6e417d418097f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at batch 1000 / 18493  = 1.9773715734481812\n",
      "Loss at batch 2000 / 18493  = 1.197084665298462\n",
      "Loss at batch 3000 / 18493  = 1.0590198040008545\n",
      "Loss at batch 4000 / 18493  = 1.1088236570358276\n",
      "Loss at batch 5000 / 18493  = 1.2102621793746948\n",
      "Loss at batch 6000 / 18493  = 1.2789114713668823\n",
      "Loss at batch 7000 / 18493  = 0.8305197358131409\n",
      "Loss at batch 8000 / 18493  = 0.9642782211303711\n",
      "Loss at batch 9000 / 18493  = 1.1025140285491943\n",
      "Loss at batch 10000 / 18493  = 0.7415385842323303\n",
      "Loss at batch 11000 / 18493  = 0.8394097685813904\n",
      "Loss at batch 12000 / 18493  = 0.7122843265533447\n",
      "Loss at batch 13000 / 18493  = 0.7062592506408691\n",
      "Loss at batch 14000 / 18493  = 0.9173645973205566\n",
      "Loss at batch 15000 / 18493  = 0.8504229187965393\n",
      "Loss at batch 16000 / 18493  = 0.697601318359375\n",
      "Loss at batch 17000 / 18493  = 0.655626654624939\n",
      "Loss at batch 18000 / 18493  = 0.8708838224411011\n",
      "Training epoch: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5631dd90483347c69f3bc20fe29e53d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at batch 1000 / 18493  = 0.6847610473632812\n",
      "Loss at batch 2000 / 18493  = 0.5171463489532471\n",
      "Loss at batch 3000 / 18493  = 0.6090697646141052\n",
      "Loss at batch 4000 / 18493  = 0.9996364712715149\n",
      "Loss at batch 5000 / 18493  = 0.6395652890205383\n",
      "Loss at batch 6000 / 18493  = 0.5863460302352905\n",
      "Loss at batch 7000 / 18493  = 0.5561689138412476\n",
      "Loss at batch 8000 / 18493  = 0.5133061408996582\n",
      "Loss at batch 9000 / 18493  = 0.7221873998641968\n",
      "Loss at batch 10000 / 18493  = 0.6266103386878967\n",
      "Loss at batch 11000 / 18493  = 0.49382537603378296\n",
      "Loss at batch 12000 / 18493  = 0.5834024548530579\n",
      "Loss at batch 13000 / 18493  = 0.7013503313064575\n",
      "Loss at batch 14000 / 18493  = 0.5959141850471497\n",
      "Loss at batch 15000 / 18493  = 0.49765536189079285\n",
      "Loss at batch 16000 / 18493  = 0.7473887801170349\n",
      "Loss at batch 17000 / 18493  = 0.4556562602519989\n",
      "Loss at batch 18000 / 18493  = 0.534711480140686\n",
      "Training epoch: 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22139f59b3f42fa806d9223862ae542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at batch 1000 / 18493  = 0.4708184599876404\n",
      "Loss at batch 2000 / 18493  = 0.3935842216014862\n",
      "Loss at batch 3000 / 18493  = 0.42989712953567505\n",
      "Loss at batch 4000 / 18493  = 0.36175981163978577\n",
      "Loss at batch 5000 / 18493  = 0.5760279893875122\n",
      "Loss at batch 6000 / 18493  = 0.49386459589004517\n",
      "Loss at batch 7000 / 18493  = 0.33755549788475037\n",
      "Loss at batch 8000 / 18493  = 0.4637730121612549\n",
      "Loss at batch 9000 / 18493  = 0.2827076315879822\n",
      "Loss at batch 10000 / 18493  = 0.41411733627319336\n",
      "Loss at batch 11000 / 18493  = 0.37108033895492554\n",
      "Loss at batch 12000 / 18493  = 0.43648576736450195\n",
      "Loss at batch 13000 / 18493  = 0.44258612394332886\n",
      "Loss at batch 14000 / 18493  = 0.4757770895957947\n",
      "Loss at batch 15000 / 18493  = 0.2883436679840088\n",
      "Loss at batch 16000 / 18493  = 0.3867112398147583\n",
      "Loss at batch 17000 / 18493  = 0.37036436796188354\n",
      "Loss at batch 18000 / 18493  = 0.34048911929130554\n"
     ]
    }
   ],
   "source": [
    "coco_dataloader = DataLoader(coco_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "epochs = 3\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=2000, num_training_steps=epochs * len(coco_dataloader))\n",
    "\n",
    "tune_caption_to_CLIP(model, tokenizer, optimizer, scheduler, coco_dataloader, epochs=epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct text from embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/rmokady/CLIP_prefix_caption/blob/main/predict.py\n",
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    tokens=None,\n",
    "    prompt=None,\n",
    "    embed=None,\n",
    "    entry_count=1,\n",
    "    entry_length=67,  # maximum number of words\n",
    "    top_p=0.8,\n",
    "    temperature=1.0,\n",
    "    stop_token: str = \".\",\n",
    "):\n",
    "    model.eval()\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "    stop_token_index = tokenizer.encode(stop_token)[0]\n",
    "    filter_value = -float(\"Inf\")\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for entry_idx in range(entry_count):\n",
    "            if embed is not None:\n",
    "                generated = embed\n",
    "            else:\n",
    "                if tokens is None:\n",
    "                    tokens = torch.tensor(tokenizer.encode(prompt))\n",
    "                    tokens = tokens.unsqueeze(0).to(device)\n",
    "\n",
    "                generated = model.transformer.wte(tokens)\n",
    "\n",
    "            for i in range(entry_length):\n",
    "                generated = generated.to(device)\n",
    "                outputs = model(inputs_embeds=generated)\n",
    "                logits = outputs.logits\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(\n",
    "                    torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1\n",
    "                )\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "                next_token = torch.argmax(logits, -1).unsqueeze(0)\n",
    "                next_token_embed = model.transformer.wte(next_token)\n",
    "                if tokens is None:\n",
    "                    tokens = next_token\n",
    "                else:\n",
    "                    tokens = torch.cat((tokens, next_token), dim=1)\n",
    "                generated = torch.cat((generated, next_token_embed), dim=1)\n",
    "                if stop_token_index == next_token.item():\n",
    "                    break\n",
    "\n",
    "            output_list = list(tokens.squeeze().cpu().numpy())\n",
    "            output_text = tokenizer.decode(output_list)\n",
    "            generated_list.append(output_text)\n",
    "\n",
    "    return generated_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode text\n",
    "text = \"A man walking his dog.\"\n",
    "\n",
    "# test_idx = 93\n",
    "# text = coco_dataset.coco_data[test_idx]\n",
    "\n",
    "print(\"Original Text:\\n\\n\", text, end=\"\\n\\n\")\n",
    "text_emb = sent_model.encode([text], convert_to_tensor=True)\n",
    "\n",
    "print(\"Reconstructed text:\", end=\"\")\n",
    "generate(model, tokenizer, embed=text_emb.view(1, 1, -1), stop_token=tokenizer.pad_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works pretty well for text that is in the training distribution; however, poor generalization as OOD inputs lead to poor reconstruction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general-gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
